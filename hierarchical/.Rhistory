classes<-matrix(1,1,totalSamples)
for (i in 1:totalSamples){
for (j in 1:(K-1)){
if (randUnif[i] > cumulativeProbs[i,j])
classes[i] <- j+1;
}
}
genData=list("actualPars"=actualPars,"features"=features,"classes"=classes)
return (genData)
}
#classes <- 1 + rbinom(n=length(linear), size=1, prob=prob)
#classes are labeled from 1 to K
#data are generated
genData1 <- generateLogisticData (D,K)
trainFeatures1 <- genData1$features[1:trainingSamples,]
testFeatures1 <-genData1$features[(trainingSamples+1):totalSamples,]
trainClasses1 <-genData1$classes[1:trainingSamples]
testClasses1 <- genData1$classes[(trainingSamples+1):totalSamples]
genData2 <- generateLogisticData (D,K)
trainFeatures2 <- genData2$features[1:trainingSamples,]
testFeatures2 <-genData2$features[(trainingSamples+1):totalSamples,]
trainClasses2 <-genData2$classes[1:trainingSamples]
testClasses2 <- genData2$classes[(trainingSamples+1):totalSamples]
#MLE estimates, currently commented out
library(ggplot2)
library(Rcpp)
scaledTrainClasses1 <- trainClasses1-1
scaledTestClasses1 <- testClasses1-1
testFrame1 = data.frame(testFeatures1)
trainFrame1=data.frame(scaledTrainClasses1,trainFeatures1)
msat1 <- multinom(scaledTrainClasses1 ~ X1 + X2 -1, data=trainFrame1)
scaledTrainClasses2 <- trainClasses2-1
scaledTestClasses2 <- testClasses2-1
testFrame2 = data.frame(testFeatures2)
trainFrame2=data.frame(scaledTrainClasses2,trainFeatures2)
msat2 <- multinom(scaledTrainClasses2 ~ X1 + X2 -1, data=trainFrame2)
trainFeatures <- rbind(trainFeatures1,trainFeatures2)
trainClasses <- rbind(trainClasses1,trainClasses2)
groupIdx <- array(1,c(length(trainClasses1)+length(trainClasses2)))
groupIdx[length(trainClasses1)+1:length(groupIdx)]=2
dataList = list(
K = K,
Ntrain = trainingSamples,
D = D,
L = 2,
trainClasses = trainClasses,
trainFeatures = trainFeatures,
groupIdx=groupIdx,
upperSigma=100
)
dataList
D<-1
#D: number of features
library(MASS)
library(nnet)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source('trainLogistic.R')
testSamples<-1000
totalSamples <- trainingSamples + testSamples
generateLogisticData <- function (D,K) {
#generate the pars
actualPars<-matrix(ncol=D,nrow = K-1)
for (i in 1:(K-1)) {
actualPars[i,]<-1*rnorm(D)
}
zeros <- rep (0,D)
actualPars<-t(rbind(zeros,actualPars))
#generate all data
#assume all features are N(0,1)
features<-matrix(ncol = D,nrow = totalSamples)
linear <-matrix(ncol=K,nrow = totalSamples)
actualProbs<-matrix(ncol=K,nrow = totalSamples)
for (i in 1:D) {
features[,i]<-rnorm(totalSamples)
}
for (i in 1:K) {
linear[,i] <- features %*% actualPars[,i]
}
exponent<-exp(linear)
for (i in 1:totalSamples){
actualProbs[i,] <- exponent[i,] / sum(exponent[i,])
}
cumulativeProbs<-actualProbs*0
cumulativeProbs[,1]<-actualProbs[,1]
for (i in 2:K) {
for (j in 1:i) {
cumulativeProbs[,i] <- cumulativeProbs[,i] + actualProbs[,j]
}
}
randUnif<-runif(totalSamples)
classes<-matrix(1,1,totalSamples)
for (i in 1:totalSamples){
for (j in 1:(K-1)){
if (randUnif[i] > cumulativeProbs[i,j])
classes[i] <- j+1;
}
}
genData=list("actualPars"=actualPars,"features"=features,"classes"=classes)
return (genData)
}
#classes <- 1 + rbinom(n=length(linear), size=1, prob=prob)
#classes are labeled from 1 to K
#data are generated
genData1 <- generateLogisticData (D,K)
trainFeatures1 <- genData1$features[1:trainingSamples,]
testFeatures1 <-genData1$features[(trainingSamples+1):totalSamples,]
trainClasses1 <-genData1$classes[1:trainingSamples]
testClasses1 <- genData1$classes[(trainingSamples+1):totalSamples]
genData2 <- generateLogisticData (D,K)
trainFeatures2 <- genData2$features[1:trainingSamples,]
testFeatures2 <-genData2$features[(trainingSamples+1):totalSamples,]
trainClasses2 <-genData2$classes[1:trainingSamples]
testClasses2 <- genData2$classes[(trainingSamples+1):totalSamples]
#MLE estimates, currently commented out
library(ggplot2)
library(Rcpp)
scaledTrainClasses1 <- trainClasses1-1
scaledTestClasses1 <- testClasses1-1
testFrame1 = data.frame(testFeatures1)
trainFrame1=data.frame(scaledTrainClasses1,trainFeatures1)
msat1 <- multinom(scaledTrainClasses1 ~ X1 + X2 -1, data=trainFrame1)
scaledTrainClasses2 <- trainClasses2-1
scaledTestClasses2 <- testClasses2-1
testFrame2 = data.frame(testFeatures2)
trainFrame2=data.frame(scaledTrainClasses2,trainFeatures2)
msat2 <- multinom(scaledTrainClasses2 ~ X1 + X2 -1, data=trainFrame2)
trainFeatures <- rbind(trainFeatures1,trainFeatures2)
trainClasses <- rbind(trainClasses1,trainClasses2)
groupIdx <- array(1,c(length(trainClasses1)+length(trainClasses2)))
groupIdx[length(trainClasses1)+1:length(groupIdx)]=2
dataList = list(
K = K,
Ntrain = trainingSamples,
D = D,
L = 2,
trainClasses = trainClasses,
trainFeatures = trainFeatures,
groupIdx=groupIdx,
upperSigma=100
)
dataList
trainFeatures
cbind(trainFeatures1,trainFeatures2)
rbind(trainFeatures1,trainFeatures2)
trainFrame1
trainClasses
rbind(trainClasses1,trainClasses2)
trainFeatures1
t(trainFeatures1)
dim(trainFeatures)
dim(trainFeatures1)
class(trainFeatures1)
c(trainClasses1,trainClasses2)
trainClasses <- c(trainClasses1,trainClasses2)
trainClasses
trainFeatures <- c(trainFeatures1,trainFeatures2)
trainFeatures <- c(trainFeatures1,trainFeatures2)
trainClasses <- c(trainClasses1,trainClasses2)
groupIdx <- array(1,c(length(trainClasses1)+length(trainClasses2)))
groupIdx[length(trainClasses1)+1:length(groupIdx)]=2
log(10)
install.packages("bma")
install.packages("BMA")
library("BMA", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
?predict.bicreg
predict.bicreg
predict.bicreg()
bicreg()
bicreg
setwd("~/Documents/devel/tutorialML/hierarchical")
install.packages("lca")
install.packages("LCA")
library(LDA)
library(LCA)
>
library(LCA)
rdirichlet(n, alpha
)
detach("package:LCA", unload=TRUE)
library(lca)
install.packages("LCA")
install.packages("poLCA")
install.packages("gtools")
library(gtools)
rdirichlet(5,c(1,1,1))
tmp<-rdirichlet(5,c(1,1,1))
class(tmp)
tasks=4, classes=3
tasks=4 classes=3
tasks=4
classes <- 3
matrix (1,classes,tasks)
array(1,c(1,2))
tmp <- array(1,c(1,2))
class(tmp)
rdirichlet(3,c(1,1,1))
st
std
runif
?runif
runif(1,1,10)
alphaVector <- array(1,c(1,classes)) #vector of ones, as long as the number of classes
marginals <- rdirichlet (tasks, alphaVector)
alphaVector
marginals
features
features=2
features
conditionals <- matrix (1, c(features, tasks, classes))
conditionals
features
tasks
classes
conditionals <- array (1, dim= (features, tasks, classes)) # mean of each conditionals
conditionals <- array (1, dim= c(features, tasks, classes)) # m
conditionals
for (currentFeat in 1:features){
for (currentClass in 1:classes){
currentMu <- runif(1,0,10) #uniformly drawn between 0 and 10
conditionals[currentFeat,,currentClass] <- rnorm (tasks, currentMu) #sample the mean of each task given the classfrom a normal with mean currentMu
}
}
conditionals
?rnorm
samples=1000
features=2
tasks=4
classes=3
library(gtools)
#marginal on the class variable: samples from the Dirichlet
alphaVector <- array(1,c(1,classes)) #vector of ones, as long as the number of classes
marginals <- rdirichlet (tasks, alphaVector) # draws the marginal of each tas
marginals
alphaVector
std <- 1 #fixed std for all conditionals
conditionals <- array (1, dim= c(features, tasks, classes)) # mean of each conditionals
for (currentFeat in 1:features){
for (currentClass in 1:classes){
currentMu <- runif(1,0,10) #uniformly drawn between 0 and 10
conditionals[currentFeat,,currentClass] <- rnorm (tasks, currentMu) #sample the mean of each task given the classfrom a normal with mean currentMu
}
}
currentMu
conditionals
data <- matrix (nrows = tasks * samples, ncols = features + 2 )
?"matrix"
data <- matrix (nrow = tasks * samples, ncol = features + 2 )
data
tasks
samples
ncol
counter <-1
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
]data[counter,1] <- currentTask
]data[counter,1]
data[counter,1]
data[counter,]
TMP
tmp
View(marginals)
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
tmp
View(marginals)
View(marginals)
data[counter,]
currentClass
currentClass
conditionals[currentFeat,currentTask,]
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
logisticGeneration()
rnorm(1,conditionals[currentFeat,currentTask,currentClass])
conditionals[currentFeat,currentTask,currentClass]
logisticGeneration()
currentTask
currentTask
currentClass
currentFeat
conditionals
conditionals[currentFeat,currentTask,currentClass]
conditionals[2,1,2]
currentFeat
currentTask
currentClass
data(counter,2)
data(counter,2)
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
data
counter
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
data
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
debugSource('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
logisticGeneration()
data
library(gtools) #sampling from Dirichlet
library(e1071) #naive Bayes
#marginal on the class variable: samples from the Dirichlet
alphaVector <- array(1,c(1,classes)) #vector of ones, as long as the number of classes
marginals <- rdirichlet (tasks, alphaVector) # draws the marginal of each task
std <- 1 #fixed std for all conditionals
conditionals <- array (1, dim= c(features, tasks, classes)) # mean of each conditionals
for (currentFeat in 1:features){
for (currentClass in 1:classes){
currentMu <- runif(1,0,10) #uniformly drawn between 0 and 10
conditionals[currentFeat,,currentClass] <- rnorm (tasks, currentMu) #sample the mean of each task given the classfrom a normal with mean currentMu
}
}
data <- matrix (nrow = tasks * samples, ncol = features + 2 )
#generate the data. First column: task ID, second column: class; then the features
counter <-1
for (currentTask in 1:tasks){
for (i in 1:samples){
data[counter,1] <- currentTask
tmp <-runif(1)
#inefficient code, moreover hard-coded for 3 classes only
if ( tmp < marginals[currentTask, 1] ){
data[counter,2] <- 1
}
if ( tmp > marginals[currentTask, 1] & tmp < marginals [currentTask, 2]){
data[counter,2] <- 2
}
if ( tmp > marginals[currentTask, 2]){
data[counter,2] <- 3
}
currentClass <- data[counter,2]
for (currentFeat in 1:features) {
data [counter,2+currentFeat]  <- rnorm(1,conditionals[currentFeat,currentTask,currentClass])
}
counter <- counter + 1
}
}
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
source('~/Dropbox/research/working-papers/transfer/logisticGeneration.R')
samples
features
tasks
classes
library(gtools) #sampling from Dirichlet
library(e1071) #naive Bayes
#marginal on the class variable: samples from the Dirichlet
alphaVector <- array(1,c(1,classes)) #vector of ones, as long as the number of classes
marginals <- rdirichlet (tasks, alphaVector) # draws the marginal of each task
std <- 1 #fixed std for all conditionals
conditionals <- array (1, dim= c(features, tasks, classes)) # mean of each conditionals
for (currentFeat in 1:features){
for (currentClass in 1:classes){
currentMu <- runif(1,0,10) #uniformly drawn between 0 and 10
conditionals[currentFeat,,currentClass] <- rnorm (tasks, currentMu) #sample the mean of each task given the classfrom a normal with mean currentMu
}
}
data <- matrix (nrow = tasks * samples, ncol = features + 2 )
#generate the data. First column: task ID, second column: class; then the features
counter <-1
for (currentTask in 1:tasks){
for (i in 1:samples){
data[counter,1] <- currentTask
tmp <-runif(1)
#inefficient code, moreover hard-coded for 3 classes only
if ( tmp < marginals[currentTask, 1] ){
data[counter,2] <- 1
}
if ( tmp > marginals[currentTask, 1] & tmp < marginals [currentTask, 2]){
data[counter,2] <- 2
}
if ( tmp > marginals[currentTask, 2]){
data[counter,2] <- 3
}
currentClass <- data[counter,2]
for (currentFeat in 1:features) {
data [counter,2+currentFeat]  <- rnorm(1,conditionals[currentFeat,currentTask,currentClass])
}
counter <- counter + 1
}
}
data
classifier1 <- naiveBayes(data[1:samples,3:3+features], data[1:samples,2])
library(e1071)
install.packages(e1071)
install.packages("e1071")
library(e1071)
classifier1 <- naiveBayes(data[1:samples,3:3+features], data[1:samples,2])
classifier1 <- naiveBayes(data[1:samples,3:(3+features)], data[1:samples,2])
data[1:samples,]
classifier1 <- naiveBayes(data[1:samples,3:(3+features-1)], data[1:samples,2])
classifier1
marginals
conditionals
y = data[1:samples, 2]
y
hist(y)
hist(data[1:1000,2])
marginals[1, ]
marginals
marginals
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginal[currentClass,]
}
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginals[currentClass,]
}
marginals
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginals[currentClass,]
}
marginals
marginals <- rdirichlet (tasks, alphaVector)
marginals
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginals[currentClass,]
}
marginals
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginals[currentClass,]
}
marginals
marginals <- rdirichlet (tasks, alphaVector)
marginals
for (currentClass in 2:classes){
marginals[currentClass,] <- marginals[currentClass-1,] +  marginals[currentClass,]
}
marginals
marginals <- rdirichlet (tasks, alphaVector)
for (currentClass in 2:classes){
marginals[,currentClass] <- marginals[,currentClass-1] +  marginals[,currentClass]
}
marginals <- rdirichlet (tasks, alphaVector)
for (currentClass in 2:classes){
marginals[,currentClass] <- marginals[,currentClass-1] +  marginals[,currentClass]
}
marginals
library(gtools) #sampling from Dirichlet
library(e1071) #naive Bayes
#marginal on the class variable: samples from the Dirichlet
alphaVector <- array(1,c(1,classes)) #vector of ones, as long as the number of classes
marginals <- rdirichlet (tasks, alphaVector) # draws the marginal of each task
#now we should compute the cumulative of the marginals
for (currentClass in 2:classes){
marginals[,currentClass] <- marginals[,currentClass-1] +  marginals[,currentClass]
}
std <- 1 #fixed std for all conditionals
conditionals <- array (1, dim= c(features, tasks, classes)) # mean of each conditionals
for (currentFeat in 1:features){
for (currentClass in 1:classes){
currentMu <- runif(1,0,10) #uniformly drawn between 0 and 10
conditionals[currentFeat,,currentClass] <- rnorm (tasks, currentMu) #sample the mean of each task given the classfrom a normal with mean currentMu
}
}
data <- matrix (nrow = tasks * samples, ncol = features + 2 )
#generate the data. First column: task ID, second column: class; then the features
counter <-1
for (currentTask in 1:tasks){
for (i in 1:samples){
data[counter,1] <- currentTask
tmp <-runif(1)
#inefficient code, moreover hard-coded for 3 classes only
if ( tmp < marginals[currentTask, 1] ){
data[counter,2] <- 1
}
if ( tmp > marginals[currentTask, 1] & tmp < marginals [currentTask, 2]){
data[counter,2] <- 2
}
if ( tmp > marginals[currentTask, 2]){
data[counter,2] <- 3
}
currentClass <- data[counter,2]
for (currentFeat in 1:features) {
data [counter,2+currentFeat]  <- rnorm(1,conditionals[currentFeat,currentTask,currentClass])
}
counter <- counter + 1
}
}
data
classifier1 <- naiveBayes(data[1:samples,3:(3+features-1)], data[1:samples,2])
classifier1
marginals
conditionals
conditionals[,1,]
classifier2 <- naiveBayes(data[(samples+1):(2*samples),3:3+features], data[(samples+1):(2*samples),2])
classifier2 <- naiveBayes(data[(samples+1):(2*samples),3:3+features-1], data[(samples+1):(2*samples),2])
classifier2
marginals
conditionals[,2,]
classifier2 <- naiveBayes(data[(samples+1):(2*samples),3:3+features-1], data[(samples+1):(2*samples),2])
classifier2
conditionals[,2,]
classifier2 <- naiveBayes(data[(samples+1):(2*samples),3:(3+features-1)], data[(samples+1):(2*samples),2])
classifier2
classifier3 <- naiveBayes(data[((samples*2)+1):(3*samples),3:(3+features-1)], data[(samples*2+1):(3*samples),2])
conditionals[,3,]
classifier3 <- naiveBayes(data[((samples*2)+1):(3*samples),3:(3+features-1)], data[(samples*2+1):(3*samples),2])
classifier3
conditionals[,3,]
